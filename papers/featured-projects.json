[
    {
        "title": "Orthogonal Finetuning Made Scalable",
        "authors": "Zeju Qiu*, Weiyang Liu*, Adrian Weller, Bernhard Schölkopf",
        "venue": "EMNLP 2025",
        "description": "A parameter-efficient, memory-efficient finetuning method that works for both full-precision and quantized models.",
        "image": "papers/featured/oftv2.png",
        "link": "https://spherelab.ai/oftv2/"
    },
    {
        "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
        "authors": "Zeju Qiu, Simon Buchholz, Tim Z. Xiao, Maximilian Dax, Bernhard Schölkopf, Weiyang Liu",
        "venue": "Preprint 2025",
        "description": "A novel reparameterized training algorithm that uses orthogonal equivalence transformation to optimize large language models.",
        "image": "papers/featured/poet.png",
        "link": "https://spherelab.ai/poet/"
    },
    {
        "title": "FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models",
        "authors": "Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu",
        "venue": "Preprint 2025",
        "description": "A large-scale comprehensive Lean4 mathematical reasoning benchmark.",
        "image": "papers/featured/formalmath.png",
        "link": "https://spherelab.ai/FormalMATH/"
    },
    {
        "title": "Generating Symbolic World Models via Test-time Scaling of Large Language Models",
        "authors": "Zhouliang Yu, Yuhuan Yuan, Tim Z. Xiao, Frank Xia, Jie Fu, Ge Zhang, Ge Lin, Weiyang Liu",
        "venue": "TMLR 2025",
        "description": "A general yet effective test-time compute scaling method for large language models",
        "image": "papers/featured/ivml.png",
        "link": "https://vmlpddl.github.io/"
    },
    {
        "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
        "authors": "Zeju Qiu*, Weiyang Liu*, Haiwen Feng*, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopfu",
        "venue": "ICLR 2025 Spotlight",
        "description": "A comprehensive study to evaluate whether language models can truly understand symbolic graphics programs.",
        "image": "papers/featured/sgpbench.png",
        "link": "https://sgp-bench.github.io/"
    },
    {
        "title": "Verbalized Machine Learning: Revisiting Machine Learning with Language Models",
        "authors": "Tim Z. Xiao, Robert Bamler, Bernhard Schölkopf, Weiyang Liu",
        "venue": "TMLR 2025",
        "description": "A novel in-context machine learning paradigm that enables effective learning of text prompts in large language models.",
        "image": "papers/featured/vml.png",
        "link": "https://arxiv.org/abs/2406.04344/"
    }
]
